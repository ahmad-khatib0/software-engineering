
- let’s consider a scenario where you only perform manual testing throughout your 
  application development cycle, and see how automated testing compares in the same 
  situation. Let’s say, on average, each feature in your application has 20 test cases,
  and you take 2 minutes per test case to execute them, or 40 minutes to test one feature 
  manually. Whenever a new feature is developed, you need to test its integration with 
  the existing features and also ensure the existing features are not broken due to the
  new changes—a practice referred to as regression testing. The risk of not doing 
  regression testing early enough is that you will find integration bugs only during 
  release testing, which is very late in the cycle and might delay the release timeline. 
  So, in our example, regression testing along with new feature testing will take 80 
  minutes when there is a second feature, 120 minutes when there is a third feature... 

- JUnit, TestNG, and NUnit are some commonly adopted unit testing frameworks in the 
  backend. Jest, Mocha, and Jasmine are popular frontend unit testing frameworks.

- Contract tests are written to validate the stubs against the actual contracts of the 
  integrating services and to provide feedback continuously to both teams as they progress
  with development. Contract tests don’t necessarily check for the exact data returned
  by the integrating service, but rather focus on the contract structure itself. In our
  example ecommerce application, contract tests can be added to validate the external
  vendor PIM service’s contract so that whenever it changes, we can change the order
  service features accordingly. 

- Automated Functional Testing Strategy: 
  The test pyramid recommends having a broad bucket of micro-level tests and gradually 
  reducing the number of macro-level tests as their scope increases. For example, if 
  you have 10x unit and integration tests, you should have 5x service tests and only 
  x UI-driven tests.

- Another part of the automation strategy should be to have a way to track the automation
  coverage in order to ensure there is no backlog. Test management tools like TestRail,
  project management tools like Jira, or something as simple as an Excel sheet can be 
  adopted for this purpose.

- Agile teams follow, is to call a user story “done” only if all its micro and macro 
  level tests are automated!

- Selenium WebDriver mainly facilitates interaction with the web application rendered 
  in the browser. It doesn’t serve any other purpose, such as assertions, report 
  generation, etc., which is why we need other tools like TestNG and Maven to complete
  the automation framework.

- Selenium WebDriver has three basic components:
. APIs: These are the methods that let you interact with the application elements in the
  browser (clicking, typing in fields, etc.).
. Client library: The Selenium WebDriver client library bundles the APIs for us to 
  use in our test suite. Client libraries are available in many programming languages.
. Driver: This is the component that instructs the browser to take the actions dictated 
  by the API. The drivers are usually created and maintained by the respective browsers
  themselves and are not part of the Selenium distribution package. For example, if you
  want to run the tests against Chrome, you have to download the ChromeDriver separately
  and include it in your automation scripts.

- Java–REST Assured Framework
  REST Assured is the go-to Java library for performing automated testing of REST APIs. It 
  offers a domain-specific language (DSL) with Gherkin syntax (Given, When, Then) to create 
  readable and maintainable API tests, and uses hamcrest matchers for assertions. REST Assured
  can work with any testing framework, like JUnit or TestNG.

- Contract testing
  Pact enables the contract testing process to be fully automated. To get an idea of its
  workflow, we’ll use the same order and PIM service example. Let’s say the order service 
  is integrating with the GET /items endpoint in the external PIM service in order to retrieve 
  the item details, specifically the available sizes, SKU, and colors. The two teams’ Pact 
  workflow will be the following:
1. As the first step, the order service team collates all the integration test cases. For
   example, some of the integration test cases will be that the /items endpoint should return
   the item details as expected when the item exists, should return an empty array when the 
   item doesn’t exist, and should return appropriate error codes (404, 500, etc.) on invalid 
   requests.
2. The order service team creates stubs for these test cases using Pact.
3. The order service team writes consumer contract tests using Pact against these stubs, 
   asserting on the specific attributes: status codes, SKU, available sizes, and colors. 
   These tests, when run, automatically produce a pact file. This file captures the different 
   requests to the /items endpoint and assertions on the expected attributes in the responses.
4. The pact file is passed on to the PIM team automatically via an open source pro vision 
   called the Pact Broker, which needs to be set up and maintained by both the consumer and 
   provider teams. The Pact team also offers a paid service called Pactflow, which eliminates 
   the need to set up and maintain the Pact Broker. To make it simpler, the files can also be 
   shared via folders.
5. On the PIM service side, the team writes a provider contract test to receive the pact file
   from the Pact Broker and set up the test data in different states as per the consumer tests’
   requirements. When the provider test runs, Pact will make appropriate requests as described
   in the pact file against the actual PIM service and verify the actual responses.
6. The provider test’s results are available to the consumer via the Pact Broker, completing
   the full feedback loop without intervention. 
7. Both the consumer and the provider’s Pact tests are integrated to the CI pipeline so 
   that the teams can receive feedback continuously.

- The ice cream cone
  When you invert the test pyramid, it looks like a cone. This is referred to as the ice cream
  cone antipattern, where there are more macro-level UI-driven tests and very few micro-level
  tests. You can sense the ice cream cone antipattern when you observe some of these symptoms
  in the project:
• Waiting for a long period to get feedback from the tests run
• Catching defects later in the cycle, sometimes only during the release testing stage
• Elaborate manual testing required to give feedback despite having automated tests
• Frustration in the team with the automated tests as the diligent efforts in automating
  the UI flows have not been fruitful in giving the right results

- The cupcake
  When you duplicate tests in multiple layers, instead of a test pyramid you wind up with 
  a wide bottom layer, a wide middle, and an even wider top—overall, it looks like a 
  cupcake. This kind of disorganization generally happens when you have siloed teams of
  developers and automated testers. For example, the developers will have added unit 
  tests to verify all the invalid login inputs, and the testers will add the same tests
  in the UI layer.

- Mutation Testing 
  To find the missed test cases in unit testing, a technique called mutation testing is
  employed. Mutation testing changes the application’s code and checks if the tests fail.
  For example, when there are void method calls, it removes the calls in the code and
  runs the unit tests again. The mutation is said to be “killed” if the tests fail and 
  to have “survived” if not. PIT is a popular mutation testing tool that can be added 
  as a Maven dependency and executed from the command line. It lists the test cases 
  that survived along with an overall mutation score for the application. Mutation 
  testing, though very effective, is time-consuming; hence, it has to be used wisely.


