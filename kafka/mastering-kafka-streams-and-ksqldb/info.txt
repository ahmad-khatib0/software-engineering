-- When we say a log is ordered, what we mean is that a record’s position in the log is fixed, and never changes

-- Kafka topics are extremely flexible with what you store in them. For example, you can have homgenous 
   topics that contain only one type of data, or heterogeneous topics that contain multiple types of data.

-- A bootstrap server is the host/IP pair for one or more brokers

-- Multiple consumer groups can consume from a single topic, and each consumer 
     group processes messages independently of other consumer groups
     
-- While partitions can be added to an existing topic, the recommended pattern is to create a new source topic
     with the desired number of partitions, and to migrate all of the existing workloads to the new topic.


There are 3 basic kinds of processors in Kafka Streams:
Source processors - 
    Sources are where information flows into the Kafka Streams application.
    Data is read from a Kafka topic and sent to one or more stream processors.
  
Stream processors - 
    These processors are responsible for applying data processing / transformation logic
    on the input stream. In the high-level DSL, these processors are defined 
    using a set of built-in operators that are exposed by the Kafka Streams 
    library, Some example operators are: filter, map, flatMap, and join.

Sink processors - 
    Sinks are where enriched, transformed, filtered, or otherwise processed records are written 
    back to Kafka, either to be handled by another stream processing application or to be sent 
    to a downstream datastore via something like Kafka Connect. Like source processors, 
    sink processors are connected to a Kafka topic.


-- if your topology reads from one source topic that contains 16 partitions,
    then Kafka Streams will create 16 tasks, each of which will instantiate it’s own copy
    of the underlying processor topology. Once Kafka Streams has created all of the
    tasks, it will assign the source partitions to be read from to each task.


# init a project with gradle
$ gradle init \
    --type java-application \
    --dsl groovy \
    --test-framework junit-jupiter \
    --project-name my-project \
    --package com.example


-- Kafka Streams leverages a programming paradigm called dataflow programming (DFP), which is a 
      data-centric method of representing programs as a series of inputs, outputs, and processing 
      stages. This leads to a very natural and intuitive way of creating stream processing programs...
      
      Instead of building a program as a sequence of steps, the stream processing logic in a
      Kafka Streams application is structured as a directed acyclic graph (DAG), where nodes
      represent a processing step, or processor, and the edges represent input
      and output streams (where data flows from one processor to another)


--  There are two ways to model the data in your Kafka topics: 
        as a Stream (also called a record stream) 
        or a Table (also known as a changelog stream)


-- KStream: an abstraction of a partitioned record stream, in which data is represented 
      using insert semantics (i.e. each event is considered to be independent of other events).

-- KTable: an abstraction of a partitioned table (i.e. changelog stream), in which data is 
      represented using update semantics (the latest representation of a given key is
      tracked by the application). Since KTables are partitioned,
      each Kafka Streams task contains only a subset of the full table.
      
GlobalKTable: similar to a KTable, except each GlobalKTable contains a
      complete (i.e. unpartitioned) copy of the underlying data. 


 +-----------------------------+
 | -- Kafka Stream Conclusion: |
 +-----------------------------+

-- Kafka Streams lives in the stream processing layer of the Kafka ecosystem.
      This is where sophisticated data processing, transformation, and enrichment happens.

-- Kafka Streams was built to simplify the development of stream processing
      applications with a simple, functional API and a set of stream processing
      primitives that can be reused across projects. When more control is needed, a
      lower-level Processor API can also be used to define your topology.

-- Kafka Streams has a friendlier learning curve and a simpler deployment
      model than cluster-based solutions like Apache Flink and Apache Spark
      Streaming. It also supports event-at-a-time processing, which is considered true streaming.

-- Kafka Streams is great for solving problems that require or benefit from realtime decision-making
      and data processing. Furthermore, it is reliable, main- tainable, scalable, and elastic.





-- The simplest form of stream processing requires no memory of previously seen events. Each event 
      is consumed, processed1, and subsequently forgotten. This par- adigm is called stateless processing,

 +------------------------------------------------------------------------------------+
 | -- The distinction between stateless and stateful stream processing applications : |
 +------------------------------------------------------------------------------------+
-- In stateless applications, each event handled by your Kafka Streams application is processed 
      independently of other events, and only Stream views are needed by your 
      application In other words, your application treats each event as a self-contained 
      insert and requires no mem- ory of previously seen events.

-- Stateful applications, on the other hand, need to remember information about
      previously seen events in one or more steps of your processor topology, usually for the 
      purpose of aggregating, windowing, or joining event streams. These applications are more 
      complex under the hood since they need to track additional data, or state.


.. Some operators, like filter, are considered stateless because they only need to look at the 
      current record to perform an action (in this case, filter looks at each record individually 
      to determine whether or not the record should be forwarded to downstream processors).
      Other operators, like count, are stateful since they require knowledge of previous events
      (count needs to know how many events it has seen so far in order to track the num ber of messages).




 +--------------------------------------------------------------------------------------------+
 | // Avro is a popular format in the Kafka community, largely due to its compact byte        |
 | // representation (which is advantageous for high throughput applications), native support |
 | // for record schemas, and a schema management tool called Schema Registry                 |
 | // There are other advantages, as well. For example, some Kafka Connectors can             |
 | // use Avro schemas to automatically infer the table structure of downstream data stores,  |
 | // so encoding our output records in this format can help with data integration downstream.|
 +--------------------------------------------------------------------------------------------+

in avro file: 

"namespace": "com.magicalpipelines.model",
# The desired package name for your data class.

"name": "EntitySentiment",
# The name of the Java class that will contain the Avro-based data model. 
# This class will be used in subsequent stream processing steps.
