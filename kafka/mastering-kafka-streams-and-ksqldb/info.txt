-- When we say a log is ordered, what we mean is that a record’s position in the log is fixed, and never changes

-- Kafka topics are extremely flexible with what you store in them. For example, you can have homgenous 
   topics that contain only one type of data, or heterogeneous topics that contain multiple types of data.

-- A bootstrap server is the host/IP pair for one or more brokers

-- Multiple consumer groups can consume from a single topic, and each consumer 
     group processes messages independently of other consumer groups
     
-- While partitions can be added to an existing topic, the recommended pattern is to create a new source topic
     with the desired number of partitions, and to migrate all of the existing workloads to the new topic.


There are 3 basic kinds of processors in Kafka Streams:
Source processors - 
    Sources are where information flows into the Kafka Streams application.
    Data is read from a Kafka topic and sent to one or more stream processors.
  
Stream processors - 
    These processors are responsible for applying data processing / transformation logic
    on the input stream. In the high-level DSL, these processors are defined 
    using a set of built-in operators that are exposed by the Kafka Streams 
    library, Some example operators are: filter, map, flatMap, and join.

Sink processors - 
    Sinks are where enriched, transformed, filtered, or otherwise processed records are written 
    back to Kafka, either to be handled by another stream processing application or to be sent 
    to a downstream datastore via something like Kafka Connect. Like source processors, 
    sink processors are connected to a Kafka topic.


-- if your topology reads from one source topic that contains 16 partitions,
    then Kafka Streams will create 16 tasks, each of which will instantiate it’s own copy
    of the underlying processor topology. Once Kafka Streams has created all of the
    tasks, it will assign the source partitions to be read from to each task.


# init a project with gradle
$ gradle init \
    --type java-application \
    --dsl groovy \
    --test-framework junit-jupiter \
    --project-name my-project \
    --package com.example


-- Kafka Streams leverages a programming paradigm called dataflow programming (DFP), which is a 
      data-centric method of representing programs as a series of inputs, outputs, and processing 
      stages. This leads to a very natural and intuitive way of creating stream processing programs...
      
      Instead of building a program as a sequence of steps, the stream processing logic in a
      Kafka Streams application is structured as a directed acyclic graph (DAG), where nodes
      represent a processing step, or processor, and the edges represent input
      and output streams (where data flows from one processor to another)


--  There are two ways to model the data in your Kafka topics: 
        as a Stream (also called a record stream) 
        or a Table (also known as a changelog stream)


-- KStream: an abstraction of a partitioned record stream, in which data is represented 
      using insert semantics (i.e. each event is considered to be independent of other events).

-- KTable: an abstraction of a partitioned table (i.e. changelog stream), in which data is 
      represented using update semantics (the latest representation of a given key is
      tracked by the application). Since KTables are partitioned,
      each Kafka Streams task contains only a subset of the full table.
      
GlobalKTable: similar to a KTable, except each GlobalKTable contains a
      complete (i.e. unpartitioned) copy of the underlying data. 


 +-----------------------------+
 | -- Kafka Stream Conclusion: |
 +-----------------------------+

-- Kafka Streams lives in the stream processing layer of the Kafka ecosystem.
      This is where sophisticated data processing, transformation, and enrichment happens.

-- Kafka Streams was built to simplify the development of stream processing
      applications with a simple, functional API and a set of stream processing
      primitives that can be reused across projects. When more control is needed, a
      lower-level Processor API can also be used to define your topology.

-- Kafka Streams has a friendlier learning curve and a simpler deployment
      model than cluster-based solutions like Apache Flink and Apache Spark
      Streaming. It also supports event-at-a-time processing, which is considered true streaming.

-- Kafka Streams is great for solving problems that require or benefit from realtime decision-making
      and data processing. Furthermore, it is reliable, main- tainable, scalable, and elastic.





-- The simplest form of stream processing requires no memory of previously seen events. Each event 
      is consumed, processed1, and subsequently forgotten. This par- adigm is called stateless processing,

 +------------------------------------------------------------------------------------+
 | -- The distinction between stateless and stateful stream processing applications : |
 +------------------------------------------------------------------------------------+
-- In stateless applications, each event handled by your Kafka Streams application is processed 
      independently of other events, and only Stream views are needed by your 
      application In other words, your application treats each event as a self-contained 
      insert and requires no mem- ory of previously seen events.

-- Stateful applications, on the other hand, need to remember information about
      previously seen events in one or more steps of your processor topology, usually for the 
      purpose of aggregating, windowing, or joining event streams. These applications are more 
      complex under the hood since they need to track additional data, or state.


.. Some operators, like filter, are considered stateless because they only need to look at the 
      current record to perform an action (in this case, filter looks at each record individually 
      to determine whether or not the record should be forwarded to downstream processors).
      Other operators, like count, are stateful since they require knowledge of previous events
      (count needs to know how many events it has seen so far in order to track the num ber of messages).




 +--------------------------------------------------------------------------------------------+
 | // Avro is a popular format in the Kafka community, largely due to its compact byte        |
 | // representation (which is advantageous for high throughput applications), native support |
 | // for record schemas, and a schema management tool called Schema Registry                 |
 | // There are other advantages, as well. For example, some Kafka Connectors can             |
 | // use Avro schemas to automatically infer the table structure of downstream data stores,  |
 | // so encoding our output records in this format can help with data integration downstream.|
 +--------------------------------------------------------------------------------------------+

in avro file: 

"namespace": "com.magicalpipelines.model",
# The desired package name for your data class.

"name": "EntitySentiment",
# The name of the Java class that will contain the Avro-based data model. 
# This class will be used in subsequent stream processing steps.




To support stateful operations, we need a way of storing and retrieving the remembered data, or state, 
    required by each stateful operator in our application (e.g. count, aggregate, join, etc). 
    The storage abstraction that addresses these needs in Kafka Streams is called a state store, 
    and since a single Kafka Streams application can leverage many stateful operators, 
    a single application may contain several state stores.


One important thing to look at when deciding between using a KTable or
    GlobalKTable is the keyspace. If the keyspace is very large (i.e. has high cardinality / lots
    of unique keys), or is expected to grow into a very large keyspace, then it
    makes more sense to use a KTable so that you can distribute fragments of the entire
    state across all of your running application instances. By partitioning the state in this
    way, we can lower the local storage overhead for each individual Kafka Streams instance.

Perhaps a more important consideration when choosing between a KTable or
    GlobalKTable is whether or not you need time synchronized processing. A
    KTable is time synchronized, so when Kafka Streams is reading from multiple
    sources (e.g. in the case of a join), it will look at the timestamp to determine which
    record to process next. This means a join will reflect what the combined record would
    have been at a certain time, and this makes the join behavior more predictable. On the
    other hand, GlobalKTables are not time synchronized, and are “completely populated before 
    any processing is done”.13 Therefore, joins are always made against the
    most up-to-date version of a GlobalKTable, which changes the semantics of the program.


A GlobalKTable should be used when your keyspace is small, you want to avoid the co-partitioning
requirements of a join and when time synchronization is not needed.


Kafka Streams includes three different join operators for joining streams and tables: 
   1- Inner join. The join is triggered when the input records 
      on both sides of the join share the same key.

   2- Left join. The join semantics are different depending on the type of join:
      a- for stream-stream joins - a join is triggered when a record on the left side of the join is
         received. If there is no record with the same key on the right side of the join, then the
         right value is set to null.
      b- for stream-stream and table-table joins - same semantics as a stream-stream left join
         (see above), except an input on the right side of the join can also trigger a lookup. If
         the right side triggers the join and there is no matching key on the left side, then the
         join will not produce a result.
    
   3- Outer join. The join is triggered when a record on the either side of the join 
      is received. If there is no matching record with the same key on the opposite 
      side of the join, then the corresponding value is set to null. 

To ensure related events are routed to the same partition, we must ensure the follow-
  ing co-partitioning requirements are met:
  1- Records on both sides must be keyed by the same field, and must be partitioned on that 
     key using the same partitioning strategy
  2- The input topics on both sides of the join must contain the same number of partitions 


When we add a key-changing operator to our topology, the underlying data will be marked
    for repartitioning. This means that as soon as we add a downstream operator that reads the
    new key, Kafka Streams will:
    send the rekeyed data to an internal repartition topic re-read the newly rekeyed data back 
    into Kafka Streams This process ensures related records (i.e. records that share the same key) 
    will be processed by the same task in subsequent topology steps. However, the network trip required
    for rerouting data to a special repartition topic means that rekey operations can be expensive.